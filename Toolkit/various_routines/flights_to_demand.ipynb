{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fa9ba4",
   "metadata": {},
   "source": [
    "# Demand Generation from Flight Departures\n",
    "Author Ilias Parmaksizoglou\n",
    "\n",
    "A Jupyter notebook developed to transform historical data from Terminal 1 of Malpensa International Airport to trip starts associated with a specific time-window throughout the day and a flight departure. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b355edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iparm\\anaconda3\\envs\\mesa\\Lib\\site-packages\\geopandas\\_compat.py:124: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "C:\\Users\\iparm\\AppData\\Local\\Temp\\ipykernel_34764\\3860749697.py:6: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "# Necessary Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import calendar\n",
    "\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "from demand_generation import *\n",
    "\n",
    "# Get the current notebook's directory\n",
    "notebook_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "# Construct the path to the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded5cdab",
   "metadata": {},
   "source": [
    "First we load the necessary data relating to departures. Data are provided from SEA Aeroporti Milano and relate to the period between 1/6/2022 and 30/6/2022. The provided data contain both departing and arriving flights, but we are only interested in departures for now. The datasets loaded to the notebooks are: \n",
    "\n",
    "* mxp_data.csv (located on the data/mxp directory) - Containing arrivals/departures from MXP\n",
    "* airlines.xlsx (located on the data/mxp directory) - Containing ICAO codes for associated airlines\n",
    "\n",
    "Furthermore we define, time-window length of reference at 30 minutes. Additionally, we define factor trip_start, signalling the number of clusters in advance of their flight that most people start the trip to the airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab353ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Inputs\n",
    "dir = os.path.join(parent_dir,\"data\\\\mxp\")\n",
    "data = pd.read_csv(os.path.join(dir,'mxp_data.csv')) \n",
    "data_np = data.values # Converting pandas Dataframe to numpy\n",
    "airlines = pd.read_excel(os.path.join(dir,'airlines.xlsx'))\n",
    "\n",
    "# Clustering globals\n",
    "period = 30\n",
    "trip_start = 210//period\n",
    "tot_periods = int(1440//period)\n",
    "\n",
    "# Date related\n",
    "year = 2023 # Correct is 2022 we just assume that traffic in 2023 will be similar\n",
    "month = 6\n",
    "month_name = calendar.month_name[month] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40d4595a",
   "metadata": {},
   "source": [
    "First we want to isolate the flights that are of interest to us, as we only deal with departures. Additionally, there are some formatting related issues that are taken care of in this step. In the end we create a departures plan, containing information for all the flights that we are interested on. Stored information contain:\n",
    "\n",
    "* Departure Time in minutes from midnight\n",
    "* Departure Cluster - Time window that the flight departs\n",
    "* Departure Mu - Time window that the average user will start their trip towards the airport for associate flight\n",
    "* Flight Code - Generated identifier for each flight\n",
    "* Date - Date of flight departure\n",
    "* Pax - Passengers associated to that flight\n",
    "* Company - Airline operating that flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa73ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty dictionary to store information\n",
    "departure_mod = {'idxs' : [],'Departure Time (min)': [], 'Departure Cluster': [], 'Departure Mu' : [], 'Flight Code' : [], \"SCHENGEN\" : [], \"TYPE\" : [], 'NATIONAL' : []}\n",
    "arrival_mod = {'idxs' : [],'Arrival Time (min)': [], 'Arrival Cluster': [], 'Flight Code' : []}\n",
    "\n",
    "# Start iterating across all flights\n",
    "for idx,row in enumerate(data_np):\n",
    "\n",
    "   \n",
    "    # Departure time is written as string in the 00:00 format but without ':'\n",
    "    timestamp = str(row[0])\n",
    "    \n",
    "    # Minutes are the last two digits of the timestamp regardless\n",
    "    minutes = int(timestamp[-2:])\n",
    "    \n",
    "    # Hours are zero if length of string smaller-equal to two\n",
    "    hours = 0 \n",
    "    if len(timestamp) >2:\n",
    "        hours = int(timestamp[0:-2])\n",
    "\n",
    "    # Departure from midnight in minutes and clusters\n",
    "    flight_arr_dep = 60*hours + minutes\n",
    "    cluster = flight_arr_dep//period\n",
    "\n",
    "    # Examined day without year and month - row[1] contains datetime\n",
    "    day = datetime.datetime.strptime(row[1], '%d/%m/%Y').date().day\n",
    "\n",
    "    # Construct specialized code for flight based on Airline|Day|Departure Time|Random ID  \n",
    "    flight_code=airlines.loc[airlines['Airline'] == row[4]].values[0][1]+'|'+str(day)+'|'+str(flight_arr_dep)+'|'+row[3]+str(idx%100)\n",
    "    schengen = (airlines[airlines['Airline'] == row[4]]['SCHENGEN']).values[0]\n",
    "    type_f = (airlines[airlines['Airline'] == row[4]]['TYPE']).values[0]\n",
    "    national = False\n",
    "    if schengen == 1:\n",
    "        schengen = False\n",
    "    else:\n",
    "        val = random.random()\n",
    "        if val <0.15:\n",
    "            schengen = False\n",
    "        else:\n",
    "            schengen = True\n",
    "            if type_f == 'LEISURE':\n",
    "                val = random.random()\n",
    "                if val <0.5:\n",
    "                    schengen = False\n",
    "                else:\n",
    "                    if random.random() <0.8:\n",
    "                        national = True\n",
    "\n",
    "            if type_f  == 'LC':\n",
    "                if random.random() <0.5:\n",
    "                    national = True \n",
    "\n",
    "            if type_f  == 'LEG':\n",
    "                if random.random() <0.01:\n",
    "                    national = True  \n",
    "    # Omit Departures - row[3] associated with flight type\n",
    "    if row[3] == \"A\":\n",
    "        # Save only Departures to the dictionary\n",
    "        arrival_mod['idxs'].append(idx)\n",
    "        arrival_mod['Arrival Time (min)'].append(flight_arr_dep)\n",
    "        arrival_mod['Arrival Cluster'].append(cluster)\n",
    "        arrival_mod['Flight Code'].append(flight_code)\n",
    "    else:         \n",
    "        # Associate departure time with trip_start factor - Insert gross assumption when flight leads to the previous day\n",
    "        mu = cluster-trip_start\n",
    "        if type_f != \"LEG\" or national == True:\n",
    "            mu+=1\n",
    "        if not(schengen):\n",
    "            mu-=2\n",
    "\n",
    "        if mu < 0:\n",
    "            mu = 0 # We could use (mu = mu + 60*24/period) but then Date of trip start and Flight would differ - potentially problematic\n",
    "\n",
    "        # Save only Departures to the dictionary\n",
    "        departure_mod['idxs'].append(idx)\n",
    "        departure_mod['Departure Time (min)'].append(flight_arr_dep)\n",
    "        departure_mod['Departure Cluster'].append(cluster)\n",
    "        departure_mod['Departure Mu'].append(int(mu))\n",
    "        departure_mod['Flight Code'].append(flight_code)\n",
    "        departure_mod['SCHENGEN'].append(schengen)\n",
    "        departure_mod['TYPE'].append(type_f)\n",
    "        departure_mod['NATIONAL'].append(national)\n",
    "\n",
    "# Create Dataframe from dictionary with idxs as index\n",
    "departures = pd.DataFrame(departure_mod)\n",
    "departures.set_index('idxs',inplace=True)\n",
    "departures.index.name = None\n",
    "\n",
    "arrivals = pd.DataFrame(arrival_mod)\n",
    "arrivals.set_index('idxs',inplace=True)\n",
    "arrivals.index.name = None\n",
    "\n",
    "# Merge Dataframe with loaded plan, sort by Date and remove unnecessary information\n",
    "departures_plan = pd.concat([departures, data.loc[departure_mod['idxs']]], axis=1, join='inner')\n",
    "departures_plan = departures_plan.sort_values([\"Date\",'Time'])\n",
    "departures_plan['Date'] = departures_plan['Date'].str.replace(\"2022\",\"2023\").str.replace(\"06\",\"6\") # Apply year 'correction' and month correction\n",
    "departures_plan = departures_plan.drop(['Type','Time'],axis=1)\n",
    "departures_plan.reset_index(inplace=False)\n",
    "\n",
    "arrivals_plan = pd.concat([arrivals, data.loc[arrival_mod['idxs']]], axis=1, join='inner')\n",
    "arrivals_plan = arrivals_plan.sort_values([\"Date\",'Time'])\n",
    "arrivals_plan['Date'] = arrivals_plan['Date'].str.replace(\"2022\",\"2023\").str.replace(\"06\",\"6\") # Apply year 'correction' and month correction\n",
    "arrivals_plan = arrivals_plan.drop(['Type','Time'],axis=1)\n",
    "arrivals_plan.reset_index(inplace=False)\n",
    "\n",
    "# Save Departure plan to MXP folder\n",
    "path = f\"{dir}\\\\{month_name}\"\n",
    "exists = os.path.exists(path)\n",
    "\n",
    "# Create a new directory because it does not exist\n",
    "if not exists:\n",
    "    os.makedirs(path)\n",
    "\n",
    "departures_plan.to_csv(f\"{path}\\\\departures_plan.csv\",index=False)\n",
    "\n",
    "# df_category_A = departures_plan[departures_plan['SCHENGEN'] == True]\n",
    "# df_category_A = df_category_A[df_category_A['TYPE'] == 'LEISURE']\n",
    "# df_category_A_I = df_category_A[df_category_A['NATIONAL'] == True]\n",
    "# df_category_A_N = df_category_A[df_category_A['NATIONAL'] == False]\n",
    "\n",
    "# df_category_B = departures_plan[departures_plan['SCHENGEN'] == False]\n",
    "# df_category_B = df_category_B[df_category_B['TYPE'] == 'LEISURE']\n",
    "\n",
    "# df_category_C = departures_plan[departures_plan['SCHENGEN'] == True]\n",
    "# df_category_C = df_category_C[df_category_C['TYPE'] == 'LEG']\n",
    "# df_category_C_I = df_category_C[df_category_C['NATIONAL'] == True]\n",
    "# df_category_C_N = df_category_C[df_category_C['NATIONAL'] == False]\n",
    "\n",
    "# df_category_D = departures_plan[departures_plan['SCHENGEN'] == False]\n",
    "# df_category_D = df_category_D[df_category_D['TYPE'] == 'LEG']\n",
    "\n",
    "# df_category_E = departures_plan[departures_plan['SCHENGEN'] == True]\n",
    "# df_category_E = df_category_E[df_category_E['TYPE'] == 'LC']\n",
    "# df_category_E_I = df_category_E[df_category_E['NATIONAL'] == True]\n",
    "# df_category_E_N = df_category_E[df_category_E['NATIONAL'] == False]\n",
    "\n",
    "# df_category_F = departures_plan[departures_plan['SCHENGEN'] == False]\n",
    "# df_category_F = df_category_F[df_category_F['TYPE'] == 'LC']\n",
    "\n",
    "# # Calculate the sum of the 'Value' column for each DataFrame\n",
    "# sum_A = df_category_A['Pax'].sum()\n",
    "# sum_B = df_category_B['Pax'].sum()\n",
    "# sum_C = df_category_C['Pax'].sum()\n",
    "# sum_D = df_category_D['Pax'].sum()\n",
    "# sum_E = df_category_E['Pax'].sum()\n",
    "# sum_F = df_category_F['Pax'].sum()\n",
    "\n",
    "# sum_A_I = df_category_A_I['Pax'].sum()\n",
    "# sum_A_N = df_category_A_N['Pax'].sum()\n",
    "# sum_C_I = df_category_C_I['Pax'].sum()\n",
    "# sum_C_N = df_category_C_N['Pax'].sum()\n",
    "# sum_E_I = df_category_E_I['Pax'].sum()\n",
    "# sum_E_N = df_category_E_N['Pax'].sum()\n",
    "\n",
    "# print(sum_A/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F),sum_B/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F))\n",
    "# print(sum_C/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F),sum_D/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F))\n",
    "# print(sum_E/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F),sum_F/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F))\n",
    "\n",
    "# print(sum_A_I/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F),sum_A_N/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F))\n",
    "# print(sum_C_I/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F),sum_C_N/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F))\n",
    "# print(sum_E_I/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F),sum_E_N/(sum_A+sum_B+sum_C+sum_D+sum_E+sum_F))\n",
    "\n",
    "arrivals_plan.to_csv(f\"{path}\\\\arrivals_plan.csv\",index=False)\n",
    "total_flights = pd.concat([departures_plan,arrivals_plan],axis=0)\n",
    "total_flights.to_csv(f\"{path}\\\\total_flights_plan.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce190a05",
   "metadata": {},
   "source": [
    "Now we have to isolate the departures per day of the examined month. We will individual plans for all the days of the examined month and save them within that directory. This will be the building blocks to create the window trip start to flight matrix that estimates incoming demand per time-window for all flights within the examined day. To create that demand we will essentially sample from a normal distribution centered on the associated $\\mu$ of each flight, and a variance of a single cluster. Finally we split the traffic to Milano Metropolitan Area originating and rest of the traffic. Based on reported data from SEA, the modal share of Milano is roughly 50 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_choice(percent=50):\n",
    "    return random.randrange(100) < percent\n",
    "\n",
    "# Get number of days within examined month\n",
    "num_days = calendar.monthrange(year, month)[1]\n",
    "no_gates = 72\n",
    "gates = list(range(1,no_gates+1))\n",
    "# Align days with single digit format on dataset\n",
    "days = [datetime.date(year, month, day).strftime('X%d/X%m/%Y').replace('X0','X').replace('X','') for day in range(1, 11)]\n",
    "\n",
    "for day in days:\n",
    "\n",
    "    # Isolate Plan for examined day\n",
    "    day_plan = total_flights[total_flights[\"Date\"]==day]\n",
    "    day_plan = day_plan.reset_index(drop=True)\n",
    "    day_plan = day_plan.sort_values(by=['Departure Time (min)','Arrival Time (min)', \"SCHENGEN\"])\n",
    "\n",
    "    assigned_gates = []\n",
    "    passport_needed = []\n",
    "    for idx, f in day_plan.iterrows():\n",
    "        gate = random.choice(gates)\n",
    "\n",
    "        assigned_gates.append(gate)\n",
    "        if gate not in range(1,25):\n",
    "            gates.remove(gate)\n",
    "        \n",
    "        if gates == range(1,25):\n",
    "            gates = list(range(1,no_gates+1))\n",
    "\n",
    "        if not(f[-1]): \n",
    "            passport_needed.append(1)\n",
    "        else:\n",
    "            passport_needed.append(0)\n",
    "    \n",
    "    day_plan[\"Gate\"] = assigned_gates\n",
    "    day_plan[\"Passport\"] = passport_needed\n",
    "\n",
    "    # Define path of saved day plane\n",
    "    path = f\"{dir}\\\\{month_name}\\\\{day.replace('/','-')}\"\n",
    "    exists = os.path.exists(path)\n",
    "\n",
    "    # Create a new directory because it does not exist\n",
    "    if not exists:\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    # Save file to csv\n",
    "    departures_plan = day_plan.drop(columns=['Arrival Time (min)','Arrival Cluster'])   \n",
    "    departures_plan =departures_plan.dropna(axis=0)\n",
    "    departures_plan = departures_plan.reset_index(drop=False)\n",
    "    departures_plan = departures_plan[['Flight Code','Date','Company','Pax','Gate','Passport','Departure Time (min)','Departure Cluster','Departure Mu','index', 'SCHENGEN', 'TYPE', 'NATIONAL']]\n",
    "    departures_plan.to_csv(f\"{path}//departures_plan.csv\",index=False)\n",
    "\n",
    "    arrivals_plan = day_plan.drop(columns=['Departure Time (min)','Departure Cluster','Departure Mu'])\n",
    "    arrivals_plan =arrivals_plan.dropna(axis=0)\n",
    "    arrivals_plan = arrivals_plan.reset_index(drop=False)\n",
    "    arrivals_plan = arrivals_plan[['Flight Code','Date','Company','Pax','Gate','Passport','Arrival Time (min)','Arrival Cluster','index']]   \n",
    "    arrivals_plan.to_csv(f\"{path}//arrivals_plan.csv\",index=False)\n",
    "\n",
    "    # We define the number of passengers belonging to a departure mu across all flights\n",
    "    incoming = departures_plan.groupby([\"Departure Mu\"])[\"Pax\"].sum()\n",
    "    outgoing = arrivals_plan.groupby([\"Arrival Cluster\"])[\"Pax\"].sum()\n",
    "    \n",
    "    flight_to_arrival= np.zeros((arrivals_plan.shape[0],tot_periods))\n",
    "\n",
    "    for idx, r in arrivals_plan.iterrows():\n",
    "        flight_to_arrival[idx][int(r[-2])]= r[4] # r[-2] is arrival cluster\n",
    "\n",
    "    # Save distribution to trip matrix\n",
    "    arrival_matrix = pd.DataFrame(data = flight_to_arrival, \n",
    "                    index = arrivals_plan['Flight Code'].tolist(), \n",
    "                    columns = list(range(1,tot_periods+1)))\n",
    "\n",
    "    # Assigned demand to a flight based on its traffic\n",
    "    flight_to_departure = np.zeros((departures_plan.shape[0],tot_periods))\n",
    "\n",
    "    # Loop over all time windows of the day in incoming\n",
    "    for mu,value in incoming.items():\n",
    "\n",
    "        # Define distribution of passengers around that mu\n",
    "        incoming_distribution = np.random.normal(mu, 1, value).round().astype(int)\n",
    "\n",
    "        # Isolate flights that share that mu\n",
    "        flights_mu = departures_plan[departures_plan[\"Departure Mu\"]==mu]\n",
    "        pax = flights_mu.values[:,3] # Number of pax by r[3]\n",
    "\n",
    "\n",
    "        for j in list(set(incoming_distribution)):\n",
    "                      \n",
    "            # Random assignment of passengers to time-window j based on passengers on flight\n",
    "            choices = random.choices(flights_mu.index.tolist(), weights=tuple(pax), k=np.count_nonzero(incoming_distribution == j))\n",
    "\n",
    "            # Correction based on selected choices being sampled less times than they should\n",
    "            for idx2,id in enumerate(flights_mu.index.tolist()):\n",
    "                id2 = [x for x in choices if x == id]\n",
    "                for x in range(len(id2)):\n",
    "                    if sum(flight_to_departure[id,:]) < pax[idx2]: \n",
    "                        flight_to_departure[id,j] +=1\n",
    "                    else:\n",
    "                        pax[idx2] = 0\n",
    "\n",
    "    # Correction for flights that were sampled more time than they should\n",
    "    for idx2,id in enumerate(departures_plan.index.tolist()):                \n",
    "        if departures_plan['Pax'][idx2]>sum(flight_to_departure[id,:]):\n",
    "            mu = int(departures_plan['Departure Mu'][idx2])\n",
    "            flight_to_departure[id][mu-1]+= departures_plan['Pax'][idx2]-sum(flight_to_departure[id,:])\n",
    "\n",
    "    # Save distribution to trip matrix\n",
    "    departure_matrix = pd.DataFrame(data = flight_to_departure, \n",
    "                    index = departures_plan['Flight Code'].tolist(), \n",
    "                    columns = list(range(1,tot_periods+1)))\n",
    "    \n",
    "    # Apply Checks\n",
    "    check = departures_plan[['Pax','Flight Code']]\n",
    "    check = check.set_index('Flight Code')\n",
    "    check.index.name = None\n",
    "    check2 = departure_matrix.sum(axis=1, numeric_only=True)\n",
    "    check = check.values.reshape(check.values.shape[1],check.values.shape[0])[0]\n",
    "    assert check.any() == check2.values.any(), \"Distribution does not match reported occupancy\"\n",
    "    departure_matrix.to_csv(f\"{path}//departures_matrix.csv\")\n",
    "    arrival_matrix.to_csv(f\"{path}//arrivals_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d031dd",
   "metadata": {},
   "source": [
    "We can now continue by assigning this passengers to specific origins within Milano. First we will create demand per time-window that is region specific based on the trip matrices already developed and the we will define a direct point, number of passengers and activation time for these trips. The loop below generates all needed files for the Month of June. However, saving these files is not necessary within the simulation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop_data = pd.read_excel(f\"{parent_dir}/data/milano/milano_population_data.xlsx\")\n",
    "# varese_pop = pd.read_excel(f\"{parent_dir}/data/milano/varese_population_data.xlsx\")\n",
    "# polygons = gpd.read_file(f'{parent_dir}/data/milano/nils_milano.geojson')\n",
    "# trip_persons = [1,2,3,4,5]\n",
    "\n",
    "# for day in days:\n",
    "#     departure_flight_data = pd.read_csv(f\"{parent_dir}/data/mxp/{month_name}/{day.replace('/','-')}/departures_plan.csv\")\n",
    "#     departure_data = pd.read_csv(f\"{parent_dir}/data/mxp/{month_name}/{day.replace('/','-')}/departures_matrix.csv\",index_col=0)\n",
    "\n",
    "#     for cluster in range(1,tot_periods+1):\n",
    "#         print(cluster)\n",
    "#         demand = generate_passenger_demand(pop_data,departure_data,cluster,day.replace('/','-'),varese_pop,save=True)\n",
    "#         pass_dist_departure = demand_to_flight(day.replace('/','-'),cluster,demand,departure_data,polygons,varese_pop,departure_flight_data,trip_persons,save=True)\n",
    "#     break\n",
    "#     # arrivals_flight_data = pd.read_csv(f\"{parent_dir}/data/mxp/{month_name}/{day.replace('/','-')}/arrivals_plan.csv\")\n",
    "#     # arrivals_data = pd.read_csv(f\"{parent_dir}/data/mxp/{month_name}/{day.replace('/','-')}/arrivals_matrix.csv\",index_col=0)\n",
    "\n",
    "#     # for cluster in range(1,tot_periods+1):\n",
    "#     #     demand = generate_passenger_demand(pop_data,arrivals_data,cluster,day.replace('/','-'),varese_pop,flag='arrivals',save=True)\n",
    "#     #     pass_dist_arrival = demand_to_flight(day.replace('/','-'),cluster,demand,arrivals_data,polygons,varese_pop,arrivals_flight_data,trip_persons,flag='arrivals',save=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('mesa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dcd034c3cd7fca81abff01d6a637d135f39014f7eb4b5b4a7dafda6e7ff44908"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
